{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 00:45:58.033493: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/hassan/anaconda3/envs/Ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader , random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datasets import load_dataset\n",
    "from collections import Counter \n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from model import buildTransformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Helsinki-NLP/opus_wikipedia\", \"ar-en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = ds[\"train\"][\"translation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151136"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'ar': 'إستونيا بالإستونية رسميا جمهورية إستونيا بالإستونية هي دولة تقع في منطقة بحر البلطيق بشمال أوروبا',\n",
       "   'en': 'Estonia officially the Republic of Estonia is a country in the Baltic region of Northern Europe'},\n",
       "  {'ar': 'يحدها من الشمال خليج فنلندا ومن الغرب بحر البلطيق ومن الجنوب لاتفيا كم وإلى الشرق من بحيرة بيبوس والاتحاد الروسي كم',\n",
       "   'en': 'It is bordered to the north by the Gulf of Finland to the west by the Baltic Sea to the south by Latvia km and to the east by Lake Peipus and Russia km'},\n",
       "  {'ar': 'وعبر بحر البلطيق تقع السويد في الغرب وفنلندا في الشمال',\n",
       "   'en': 'Across the Baltic Sea lies Sweden in the west and Finland in the north'},\n",
       "  {'ar': ' ما قبل التاريخ أصبح استقرار الإنسان في إستونيا ممكنا قبل حوالي إلى سنة عندما ذاب الجليد من أخر عصر جليدي',\n",
       "   'en': 'HistoryPrehistoryHuman settlement in Estonia became possible to years ago when the ice from the last glacial era melted'},\n",
       "  {'ar': 'وفقا للالتأريخ الكاربوني تم اللاستعمار حوالي سنة مضت في بداية الألفية التاسعة قبل الميلاد',\n",
       "   'en': 'According to radiocarbon dating it was settled around years ago at the beginning of the ninth millennium BC'},\n",
       "  {'ar': 'تم العثور دلائل تشير إلى وجود مجتمعات الصيد البري والبحري حوالي سنة قبل الميلاد بالقرب من بلدة كوندا شمالي إستونيا',\n",
       "   'en': 'Evidence has been found of hunting and fishing communities existing around BC near the town of Kunda in northern Estonia'},\n",
       "  {'ar': 'أعمال يدوية من العظام والحجارة شبيهة بالتي تم إيجادها في كوندا وجدت أيضا في أماكن أخرى من إستونيا وأيضا في لاتفيا شمالي ليتوانيا وجنوبي فنلندا',\n",
       "   'en': 'Bone and stone artefacts similar to those found at Kunda have been discovered elsewhere in Estonia as well as in Latvia northern Lithuania and in southern Finland'},\n",
       "  {'ar': 'هذه الكلمة الوحيدة المسجلة من لغتهم من العصور القديمة',\n",
       "   'en': 'This is the only word of their language recorded from antiquity'},\n",
       "  {'ar': 'تم استدعاء السفن الشراعية منها سفن القراصنة التي كتبها هنري ليفونيا في سجلات له اللاتينية منذ بداية القرن ',\n",
       "   'en': 'Their sailing vessels were called pirate ships by Henry of Livonia in his Latin chronicles written at the beginning of the th century'},\n",
       "  {'ar': 'وقعت الغارة ربما الأكثر شهرة من قبل قراصنة في عام مع الهجوم على بلدة السويدية من سيغتونا من قبل المغيرين من و',\n",
       "   'en': 'Perhaps the most famous raid by Oeselian pirates occurred in with the attack on the Swedish town of Sigtuna by Finnic raiders from Couronia and Oesel'},\n",
       "  {'ar': 'وكان في السابق سفينة حربية وهذه الأخيرة أساسا سفينة تجارية',\n",
       "   'en': 'The former was a warship the latter mainly a merchant ship'},\n",
       "  {'ar': 'ودعا الله متفوق من كما وصفها هنري ليفونيا ',\n",
       "   'en': 'The superior god of Oeselians as described by Henry of Livonia was called Tharapita'},\n",
       "  {'ar': 'وفقا للأسطورة في وقائع ولدت في أحد الجبال التي تغطيها الغابات في فيروما اللاتينية استونيا البر الرئيسى من حيث طار إلى ساريما الاسم وقد فسر بأنها مساعدة',\n",
       "   'en': 'According to the legend in the chronicle Tharapita was born on a forested mountain in Virumaa mainland Estonia from where he flew to Oesel Saaremaa The name Taarapita has been interpreted as Taara help'},\n",
       "  {'ar': 'استطاع أن يجمع جيشا من من الرجال الاستونية مقاطعات مختلفة لكنه قتل خلال معركة عيد القديس ماثيو في سبتمبر ايلول ',\n",
       "   'en': 'He managed to assemble an army of Estonian men from different counties but he was killed during the Battle of St Matthews Day in September '},\n",
       "  {'ar': 'تم تشكيل دوقية استونيا في الأجزاء الشمالية من البلادك سلطان مباشرة من ملك الدنمارك من حتى عندما تم بيعه لأمر توتوني وأصبحت جزءا من ',\n",
       "   'en': 'The Duchy of Estonia was created out of the northern parts of the country and was a direct dominion of the King of Denmark from until when it was sold to the Teutonic Order and became part of the Ordenstaat'}],\n",
       " 3644)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleanText(text, lang=\"ar\"):\n",
    "    text = text.strip()  \n",
    "    if lang == \"ar\":\n",
    "        text = re.sub(r'[^\\u0621-\\u064A\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "    elif lang == \"en\":\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "dataSet = [data for data in dataSet if data[\"ar\"].replace(\" \", \"\") != data[\"en\"].replace(\" \", \"\")]\n",
    "for data in dataSet:\n",
    "    data[\"ar\"] = cleanText(data[\"ar\"], lang=\"ar\")\n",
    "    data[\"en\"] = cleanText(data[\"en\"], lang=\"en\")\n",
    "\n",
    "dataSet = dataSet[:len(dataSet) // 32]\n",
    "df = pd.DataFrame(dataSet)\n",
    "ar = df[\"ar\"].tolist()\n",
    "en = df[\"en\"].tolist()\n",
    "dataSet[:15],len(dataSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacyAr = spacy.blank(\"ar\")\n",
    "spacyEn = spacy.blank(\"en\")\n",
    "\n",
    "\n",
    "tokenizerEng = lambda text: [token.text for token in spacyEn(text)]\n",
    "\n",
    "tokenizerAr = lambda text: [token.text for token in spacyAr(text)]\n",
    "\n",
    "\n",
    "def yield_tokens(data, tokenizer):\n",
    "    for text in data:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "def buildVocab(data, tokenizer):\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        yield_tokens(data, tokenizer), \n",
    "        specials=['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "    )\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocabAr = buildVocab(ar, tokenizerAr)  \n",
    "vocabEn = buildVocab(en, tokenizerEng)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataProcess(ar, en, seq_length=None):\n",
    "    data = []\n",
    "    for rawAr, rawEn in zip(ar, en):\n",
    "        tokensAr = [vocabAr['<sos>']] + [vocabAr[token] for token in tokenizerAr(rawAr)] + [vocabAr['<eos>']]\n",
    "        tokensEn = [vocabEn['<sos>']] + [vocabEn[token] for token in tokenizerEng(rawEn)] + [vocabEn['<eos>']]\n",
    "        \n",
    "        if seq_length is not None:\n",
    "            if len(tokensAr) > seq_length:\n",
    "                tokensAr = tokensAr[:seq_length]\n",
    "            else:\n",
    "                tokensAr += [vocabAr['<pad>']] * (seq_length - len(tokensAr))\n",
    "            \n",
    "            if len(tokensEn) > seq_length:\n",
    "                tokensEn = tokensEn[:seq_length]\n",
    "            else:\n",
    "                tokensEn += [vocabEn['<pad>']] * (seq_length - len(tokensEn))\n",
    "        \n",
    "        data.append((torch.tensor(tokensAr, dtype=torch.long),torch.tensor(tokensEn, dtype=torch.long)))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = dataProcess(ar,en,350)\n",
    "\n",
    "trainSize = int(0.6 * len(dataSet))\n",
    "testSize = len(dataSet) - trainSize\n",
    "trainDataset, testDataset = random_split(dataSet, [trainSize, testSize])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else  'cpu')\n",
    "batchSize = 32\n",
    "trainLoader = DataLoader(\n",
    "    trainDataset,\n",
    "    batch_size=batchSize,\n",
    "    shuffle=True\n",
    "    )\n",
    "testLoader = DataLoader(\n",
    "    testDataset,\n",
    "    batch_size=batchSize,\n",
    "    shuffle=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderBlock(\n",
       "        (Attention): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (feedForward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (residualConnection): ModuleList(\n",
       "          (0-1): 2 x ResidualConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNormalization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNormalization()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderBlock(\n",
       "        (selfAttention): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (crossAttention): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (feedForward): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (residualConnection): ModuleList(\n",
       "          (0-2): 3 x ResidualConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNormalization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNormalization()\n",
       "  )\n",
       "  (srcEmbedding): InputEmbeddings(\n",
       "    (embedding): Embedding(14732, 512)\n",
       "  )\n",
       "  (tgtEmbedding): InputEmbeddings(\n",
       "    (embedding): Embedding(11325, 512)\n",
       "  )\n",
       "  (srcPos): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tgtPos): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (linearLayer): LinearLayer(\n",
       "    (fc): Linear(in_features=512, out_features=11325, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srcVocabSize = len(vocabAr)\n",
    "tgtVocabSize = len(vocabEn)\n",
    "model = buildTransformers(\n",
    "    srcVocabSize=srcVocabSize,\n",
    "    tgtVocabSize=tgtVocabSize,\n",
    "    srcSeqLen=350,\n",
    "    tgtSeqLen=350,\n",
    "    dModel=512,\n",
    "    n=6,\n",
    "    h=4,\n",
    "    dropout=0.1,\n",
    "    dFF=2048\n",
    ")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocabEn['<pad>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Batch [50/69], Loss: 8.0625\n",
      "Epoch [1/20] | Train Loss: 7.8183 | Val Loss: 7.0433\n",
      "Epoch [2/20], Batch [50/69], Loss: 6.7695\n",
      "Epoch [2/20] | Train Loss: 6.7471 | Val Loss: 6.8144\n",
      "Epoch [3/20], Batch [50/69], Loss: 6.4126\n",
      "Epoch [3/20] | Train Loss: 6.3822 | Val Loss: 6.6371\n",
      "Epoch [4/20], Batch [50/69], Loss: 6.0634\n",
      "Epoch [4/20] | Train Loss: 6.0417 | Val Loss: 6.5431\n",
      "Epoch [5/20], Batch [50/69], Loss: 5.7578\n",
      "Epoch [5/20] | Train Loss: 5.7392 | Val Loss: 6.4750\n",
      "Epoch [6/20], Batch [50/69], Loss: 5.4847\n",
      "Epoch [6/20] | Train Loss: 5.4637 | Val Loss: 6.4747\n",
      "Epoch [7/20], Batch [50/69], Loss: 5.2310\n",
      "Epoch [7/20] | Train Loss: 5.2245 | Val Loss: 6.4869\n",
      "Epoch [8/20], Batch [50/69], Loss: 5.0014\n",
      "Epoch [8/20] | Train Loss: 4.9977 | Val Loss: 6.4493\n",
      "Epoch [9/20], Batch [50/69], Loss: 4.7895\n",
      "Epoch [9/20] | Train Loss: 4.7776 | Val Loss: 6.4501\n",
      "Epoch [10/20], Batch [50/69], Loss: 4.5833\n",
      "Epoch [10/20] | Train Loss: 4.5727 | Val Loss: 6.4323\n",
      "Epoch [11/20], Batch [50/69], Loss: 4.3753\n",
      "Epoch [11/20] | Train Loss: 4.3567 | Val Loss: 6.4585\n",
      "Epoch [12/20], Batch [50/69], Loss: 4.1657\n",
      "Epoch [12/20] | Train Loss: 4.1661 | Val Loss: 6.5041\n",
      "Epoch [13/20], Batch [50/69], Loss: 3.9872\n",
      "Epoch [13/20] | Train Loss: 3.9690 | Val Loss: 6.5375\n",
      "Epoch [14/20], Batch [50/69], Loss: 3.7721\n",
      "Epoch [14/20] | Train Loss: 3.7741 | Val Loss: 6.5165\n",
      "Epoch [15/20], Batch [50/69], Loss: 3.5820\n",
      "Epoch [15/20] | Train Loss: 3.5915 | Val Loss: 6.5189\n",
      "Epoch [16/20], Batch [50/69], Loss: 3.4250\n",
      "Epoch [16/20] | Train Loss: 3.4014 | Val Loss: 6.5156\n",
      "Epoch [17/20], Batch [50/69], Loss: 3.2366\n",
      "Epoch [17/20] | Train Loss: 3.2304 | Val Loss: 6.5640\n",
      "Epoch [18/20], Batch [50/69], Loss: 3.0645\n",
      "Epoch [18/20] | Train Loss: 3.0481 | Val Loss: 6.5895\n",
      "Epoch [19/20], Batch [50/69], Loss: 2.8842\n",
      "Epoch [19/20] | Train Loss: 2.8789 | Val Loss: 6.5902\n",
      "Epoch [20/20], Batch [50/69], Loss: 2.7053\n",
      "Epoch [20/20] | Train Loss: 2.7039 | Val Loss: 6.5966\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "def generate_mask(src, tgt, src_pad_idx, tgt_pad_idx, device):\n",
    "    # Create source mask\n",
    "    src_mask = (src != src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    # Create target mask\n",
    "    tgt_mask = (tgt != tgt_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    seq_length = tgt.size(1)\n",
    "    causal_mask = torch.tril(torch.ones(seq_length, seq_length)).bool().to(device)\n",
    "    tgt_mask = tgt_mask & causal_mask\n",
    "    \n",
    "    return src_mask, tgt_mask\n",
    "\n",
    "# Modified training loop with proper masking\n",
    "num_epochs = 20\n",
    "best_loss = float('inf')\n",
    "trainLoss = []\n",
    "valLoss = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (src, tgt) in enumerate(trainLoader):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:].reshape(-1)\n",
    "        \n",
    "        # Generate masks\n",
    "        src_mask, tgt_mask = generate_mask(src, tgt_input, vocabAr['<pad>'], vocabEn['<pad>'], device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "        output = output.contiguous().view(-1, tgtVocabSize)\n",
    "        \n",
    "        loss = criterion(output, tgt_output)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print every 50 batches\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            avg_batch_loss = total_loss / (batch_idx + 1)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(trainLoader)}], Loss: {avg_batch_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in testLoader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:].reshape(-1)\n",
    "            \n",
    "            src_mask, tgt_mask = generate_mask(src, tgt_input,vocabAr['<pad>'], vocabEn['<pad>'], device)\n",
    "            \n",
    "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "            output = output.view(-1, tgtVocabSize)\n",
    "            \n",
    "            loss = criterion(output, tgt_output)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(trainLoader)\n",
    "    avg_val_loss = val_loss / len(testLoader)\n",
    "\n",
    "    # Save best model\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"best_transformer.pth\")\n",
    "    trainLoss.append(avg_train_loss)\n",
    "    valLoss.append(avg_val_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model.load_state_dict(torch.load(\"best_transformer.pth\"))\n",
    "\n",
    "def pad_sequence(seq, max_length, pad_token):\n",
    "    if len(seq) < max_length:\n",
    "        return seq + [pad_token] * (max_length - len(seq))\n",
    "    else:\n",
    "        return seq[:max_length]\n",
    "\n",
    "\n",
    "# Improved translation function with beam search\n",
    "def translate_sentence(sentence, beam_size=3, max_length=350):\n",
    "    model.eval()\n",
    "    # Tokenize and pad source\n",
    "    tokens = [vocabAr['<sos>']] + [vocabAr[token] for token in tokenizerAr(sentence)] + [vocabAr['<eos>']]\n",
    "    tokens = pad_sequence(tokens, max_length, vocabAr['<pad>'])\n",
    "    src_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Create source mask\n",
    "    src_mask = (src_tensor != vocabAr['<pad>']).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    # Beam search initialization\n",
    "    beam = [([vocabEn['<sos>']], 0)]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        new_beam = []\n",
    "        for seq, score in beam:\n",
    "            if seq[-1] == vocabEn['<eos>']:\n",
    "                new_beam.append((seq, score))\n",
    "                continue\n",
    "                \n",
    "            tgt_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "            tgt_len = len(seq)\n",
    "            \n",
    "            # Create causal mask\n",
    "            tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len)).bool().to(device)\n",
    "            tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model(src_tensor, tgt_tensor, src_mask, tgt_mask)\n",
    "            \n",
    "            log_probs = torch.log_softmax(output[:, -1, :], dim=-1)\n",
    "            top_probs, top_indices = log_probs.topk(beam_size)\n",
    "            \n",
    "            for i in range(beam_size):\n",
    "                new_seq = seq + [top_indices[0, i].item()]\n",
    "                new_score = score + top_probs[0, i].item()\n",
    "                new_beam.append((new_seq, new_score))\n",
    "        \n",
    "        # Keep top beam_size candidates\n",
    "        new_beam.sort(key=lambda x: x[1]/len(x[0]), reverse=True)\n",
    "        beam = new_beam[:beam_size]\n",
    "    # Select best sequence\n",
    "    best_seq = max(beam, key=lambda x: x[1]/len(x[0]))[0]\n",
    "    translated = [vocabEn.lookup_token(tok) for tok in best_seq[1:-1]]\n",
    "    return ' '.join(translated)\n",
    "\n",
    "print((translate_sentence(\"\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabAr[\"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
